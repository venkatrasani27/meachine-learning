import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import multivariate_normal

class GaussianMixtureEM:
    def __init__(self, n_components, max_iter=100, tol=1e-6):
        self.n_components = n_components
        self.max_iter = max_iter
        self.tol = tol
    
    def fit(self, X):
        n_samples, n_features = X.shape
        # Initialize the parameters: means, covariances, and weights
        np.random.seed(0)
        
        # Randomly initialize the means (select random points)
        self.means = X[np.random.choice(n_samples, self.n_components, replace=False)]
        
        # Initialize the covariance matrices to identity matrices
        self.covariances = np.array([np.eye(n_features)] * self.n_components)
        
        # Initialize the mixing coefficients (weights)
        self.weights = np.ones(self.n_components) / self.n_components
        
        # Log likelihood history (to check convergence)
        log_likelihoods = []
        
        for _ in range(self.max_iter):
            # E-step: Calculate the responsibilities
            responsibilities = self._e_step(X)
            
            # M-step: Update the parameters (means, covariances, and weights)
            self._m_step(X, responsibilities)
            
            # Compute log likelihood
            log_likelihood = self._compute_log_likelihood(X)
            log_likelihoods.append(log_likelihood)
            
            # Check for convergence (if the change in log likelihood is smaller than tol)
            if len(log_likelihoods) > 1 and abs(log_likelihoods[-1] - log_likelihoods[-2]) < self.tol:
                print(f"Converged at iteration {i+1}")
                break
        
        return log_likelihoods
    
    def _e_step(self, X):
        """E-step: Calculate the responsibilities (probabilities for each data point)"""
        responsibilities = np.zeros((X.shape[0], self.n_components))
        
        for k in range(self.n_components):
            pdf = multivariate_normal.pdf(X, mean=self.means[k], cov=self.covariances[k])
            responsibilities[:, k] = self.weights[k] * pdf
        
        # Normalize the responsibilities
        responsibilities /= responsibilities.sum(axis=1)[:, np.newaxis]
        return responsibilities
    
    def _m_step(self, X, responsibilities):
        """M-step: Update the means, covariances, and weights"""
        n_samples, n_features = X.shape
        
        # Update the weights
        self.weights = responsibilities.sum(axis=0) / n_samples
        
        # Update the means
        self.means = np.dot(responsibilities.T, X) / responsibilities.sum(axis=0)[:, np.newaxis]
        
        # Update the covariances
        for k in range(self.n_components):
            diff = X - self.means[k]
            self.covariances[k] = np.dot((responsibilities[:, k] * diff.T), diff) / responsibilities[:, k].sum()
    
    def _compute_log_likelihood(self, X):
        """Compute the log likelihood of the data given the parameters"""
        log_likelihood = 0
        for k in range(self.n_components):
            pdf = multivariate_normal.pdf(X, mean=self.means[k], cov=self.covariances[k])
            log_likelihood += self.weights[k] * pdf
        return np.sum(np.log(log_likelihood))

    def predict(self, X):
        """Predict the labels (which component each point belongs to)"""
        responsibilities = self._e_step(X)
        return np.argmax(responsibilities, axis=1)


# Generate synthetic data from a mixture of two Gaussians
np.random.seed(42)
n_samples = 1000
X1 = np.random.normal(0, 1, (n_samples // 2, 2))
X2 = np.random.normal(5, 1, (n_samples // 2, 2))
X = np.vstack([X1, X2])

# Fit the Gaussian Mixture Model (GMM) using the EM algorithm
gmm = GaussianMixtureEM(n_components=2, max_iter=100, tol=1e-6)
log_likelihoods = gmm.fit(X)

# Predict the component labels
labels = gmm.predict(X)

# Plot the results
plt.figure(figsize=(8, 6))
plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis', marker='.')
plt.scatter(gmm.means[:, 0], gmm.means[:, 1], c='red', marker='x', s=100, label='Cluster Centers')
plt.title("Gaussian Mixture Model (GMM) using EM Algorithm")
plt.xlabel('Feature 1')
plt.ylabel('Feature 2')
plt.legend()
plt.show()

# Plot the log-likelihoods to check convergence
plt.figure(figsize=(8, 6))
plt.plot(log_likelihoods)
plt.title('Log-Likelihood Convergence')
plt.xlabel('Iteration')
plt.ylabel('Log-Likelihood')
plt.show()
